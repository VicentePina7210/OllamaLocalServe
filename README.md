# OllamaLocalServe - Local serve for ollama based LLM models
## This repo will allow you to pull models from the api then select the models corresponding number
### Be sure to run this on your local environment to avoid dependency errors.
